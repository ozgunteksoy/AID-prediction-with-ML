{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c45b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import itertools as itt\n",
    "\n",
    "# Library for file manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib as m\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Configuration for graphs width and layout of graphs\n",
    "sns.set_theme(style='whitegrid')\n",
    "palette='viridis'\n",
    "\n",
    "# Warnings remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Python version\n",
    "from platform import python_version\n",
    "print('Python version in this Jupyter Notebook:', python_version())\n",
    "\n",
    "# Load library versions\n",
    "import watermark\n",
    "\n",
    "# Library versions\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Library versions\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_csv = 'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\Complete_Updated_Autoimmune_Disorder_Dataset.csv'\n",
    "df = pd.read_csv(file_path_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855cfbe",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the first 5 data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c701f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the liast 5 data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2547f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f9642",
   "metadata": {},
   "source": [
    "##### Tablodaki sütun isimlerinin açıklamaları şu şekildedir:\n",
    "\n",
    "**Patient_ID:** Hastanın kimlik numarası.<br/>\n",
    "**Age:** Hastanın yaşı.<br/>\n",
    "**Gender:** Hastanın cinsiyeti (Male/Female).<br/>\n",
    "**Diagnosis:** Hastalığın teşhisi.<br/>\n",
    "**Sickness_Duration_Months:** Hastalığın süresi (ay olarak).<br/>\n",
    "**RBC_Count:** Kırmızı kan hücresi (alyuvar) sayısı.<br/>\n",
    "**Hemoglobin:** Kanın hemoglobin seviyesi.<br/>\n",
    "**Hematocrit:** Kan hacminde kırmızı kan hücrelerinin oranı.<br/>\n",
    "**MCV:** Ortalama alyuvar hacmi.<br/>\n",
    "**MCH:** Ortalama alyuvar hemoglobin miktarı.<br/>\n",
    "**MCHC:** Alyuvarlarda hemoglobin konsantrasyonu.<br/>\n",
    "**RDW:** Alyuvar dağılım genişliği.<br/>\n",
    "**Reticulocyte_Count:** Retikülosit (genç alyuvar) sayısı.<br/>\n",
    "**WBC_Count:** Beyaz kan hücresi (akyuvar) sayısı.<br/>\n",
    "**Neutrophils:** Nötrofil yüzdesi.<br/>\n",
    "**Lymphocytes:** Lenfosit yüzdesi.<br/>\n",
    "**Monocytes:** Monosit yüzdesi.<br/>\n",
    "**Eosinophils:** Eozinofil yüzdesi.<br/>\n",
    "**Basophils:** Bazofil yüzdesi.<br/>\n",
    "**PLT_Count:** Trombosit (platelet) sayısı.<br/>\n",
    "**MPV:** Ortalama trombosit hacmi.<br/>\n",
    "**ANA:** Antinükleer antikor testi sonucu.<br/>\n",
    "**Esbach:** Esbach testi sonucu (protein kaçağı için).<br/>\n",
    "**MBL_Level:** Mannoz bağlayıcı lektin seviyesi.<br/>\n",
    "**ESR:** Eritrosit sedimentasyon hızı.<br/>\n",
    "**C3:** Kompleman C3 seviyesi.<br/>\n",
    "**C4:** Kompleman C4 seviyesi.<br/>\n",
    "**CRP:** C-reaktif protein seviyesi (iltihap göstergesi).<br/>\n",
    "**Anti-dsDNA:** Çift sarmallı DNA'ya karşı antikorlar.<br/>\n",
    "**Anti-Sm:** Sm antijenine karşı antikorlar.<br/>\n",
    "**Rheumatoid factor:** Romatoid faktör seviyesi.<br/>\n",
    "**ACPA:** Anti-sitrülinlenmiş protein antikorları.<br/>\n",
    "**Anti-TPO:** Anti-tiroid peroksidaz antikorları.<br/>\n",
    "**Anti-Tg:** Anti-tiroglobulin antikorları.<br/>\n",
    "**Anti-SMA:** Anti-düz kas antikorları.<br/>\n",
    "**Low-grade fever:** Hafif dereceli ateş.<br/>\n",
    "**Fatigue or chronic tiredness:** Yorgunluk veya kronik halsizlik.<br/>\n",
    "**Dizziness:** Baş dönmesi.<br/>\n",
    "**Weight loss:** Kilo kaybı.<br/>\n",
    "**Rashes and skin lesions:** Döküntü ve deri lezyonları.<br/>\n",
    "**Stiffness in the joints:** Eklemlerde sertlik.<br/>\n",
    "**Brittle hair or hair loss:** Kırılgan saç veya saç dökülmesi.<br/>\n",
    "**Dry eyes and/or mouth:** Kuru gözler ve/veya ağız.<br/>\n",
    "**General 'unwell' feeling:** Genel bir \"hasta hissetme\" durumu.<br/>\n",
    "**Joint pain:** Eklem ağrısı.<br/>\n",
    "**Anti_enterocyte_antibodies:** Enterositlere karşı antikorlar.<br/>\n",
    "**Anti_LKM1:** Anti-larence-karaciğer-antikorları (LKM1).<br/>\n",
    "**Anti_RNP:** Ribonükleoproteinlere karşı antikorlar.<br/>\n",
    "**ASCA:** Anti-Saccharomyces cerevisiae antikorları (Crohn hastalığı ile ilişkili).<br/>\n",
    "**Anti_Ro_SSA:** Ro/SSA antijenine karşı antikorlar.<br/>\n",
    "**Anti_CBir1:** CBir1 antijenine karşı antikorlar.<br/>\n",
    "**Anti_BP230:** BP230 antijenine karşı antikorlar (bullöz hastalıklarla ilişkili).<br/>\n",
    "**Anti_tTG:** Anti-doku transglutaminaz antikorları.<br/>\n",
    "**DGP:** Deamidated gliadin peptid antikorları.<br/>\n",
    "**Anti_BP180:** BP180 antijenine karşı antikorlar (pemfigoid ile ilişkili).<br/>\n",
    "**ASMA:** Anti-düz kas antikorları (tekrar olabilir).<br/>\n",
    "**Anti_IF:** İçsel faktörlere (Intrinsic Factor) karşı antikorlar.<br/>\n",
    "**IgG_IgE_receptor:** IgG veya IgE reseptörüne karşı antikorlar.<br/>\n",
    "**Anti_SRP:** Sinyal tanıma partikülüne karşı antikorlar.<br/>\n",
    "**Anti_desmoglein_3:** Desmoglein 3'e karşı antikorlar (pemfigus ile ilişkili).<br/>\n",
    "**Anti_La_SSB:** La/SSB antijenine karşı antikorlar.<br/>\n",
    "**Anti_Jo1:** Jo-1 antijenine karşı antikorlar (dermatomiyozit ile ilişkili).<br/>\n",
    "**ANCA:** Antinötrofil sitoplazmik antikorlar.<br/>\n",
    "**Anti_centromere:** Sentromer proteinlerine karşı antikorlar.<br/>\n",
    "**Anti_desmoglein_1:** Desmoglein 1'e karşı antikorlar (pemfigus ile ilişkili).<br/>\n",
    "**EMA:** Anti-endomisyum antikorları (çölyak hastalığı ile ilişkili).<br/>\n",
    "**Anti_type_VII_collagen:** Tip VII kollajene karşı antikorlar (epidermolizis bullosa ile ilişkili).<br/>\n",
    "**C1_inhibitor:** C1 inhibitör seviyesi (kompleman sistemiyle ilişkili).<br/>\n",
    "**Anti_TIF1:** TIF1 antijenine karşı antikorlar.<br/>\n",
    "**Anti_epidermal_basement_membrane_IgA:** Epidermal bazal membrana karşı IgA antikorları.<br/>\n",
    "**Anti_OmpC:** Dış membran protein C'ye karşı antikorlar.<br/>\n",
    "**pANCA:** Perinükleer antinötrofil sitoplazmik antikorlar.<br/>\n",
    "**Anti_tissue_transglutaminase:** Anti-doku transglutaminaz (tekrar olabilir).<br/>\n",
    "**Anti_Scl_70:** Scl-70 antijenine karşı antikorlar (sistemik skleroz ile ilişkili).<br/>\n",
    "**Anti_Mi2:** Mi-2 antijenine karşı antikorlar.<br/>\n",
    "**Anti_parietal_cell:** Parietal hücrelere karşı antikorlar.<br/>\n",
    "**Progesterone_antibodies:** Progesterona karşı antikorlar.<br/>\n",
    "**Anti_Sm:** Sm antijenine karşı antikorlar (tekrar olabilir).<br/>\n",
    "**MBL:** Mannoz bağlayıcı lektin (tekrar olabilir)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4be76e",
   "metadata": {},
   "source": [
    "##### İleri Analizler için Sütun Seçme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b2cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Orijinal dosyayı yükleme\n",
    "file_path_csv = 'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\Complete_Updated_Autoimmune_Disorder_Dataset.csv'\n",
    "data_filtered = pd.read_csv(file_path_csv, dtype={\"Anti-Tg\": int, \"Anti-TPO\": int,\"Dry eyes and mouth\": int, \n",
    "                                                  \"Joint pain\": int, \"ACPA\":int})  # INteger olarak oku\n",
    "\n",
    "# İstediğiniz sütunları seçin\n",
    "selected_columns = ['Gender', 'Diagnosis', \"RBC_Count\", \"Hemoglobin\",\"Hematocrit\", \"CRP\", \"Esbach\",\n",
    "                    \"Anti-Tg\", \"Anti-TPO\", \"Dry eyes and mouth\", \"Joint pain\",\"ACPA\",\"Fatigue or chronic tiredness\"]  \n",
    "\n",
    "df = data_filtered[selected_columns].copy()\n",
    "\n",
    "\"\"\"\n",
    "# Seçilen sütunlarla yeni bir CSV dosyası oluşturma\n",
    "output_path = 'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\Selected_Columns_Autoimmune_Dataset.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Yeni CSV dosyası oluşturuldu: {output_path}\")\n",
    "\"\"\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebf223",
   "metadata": {},
   "source": [
    "## Graves ve Normal Verileri Ayırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea03810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Graves' disease\" ve \"Normal\" için veriyi filtreleme\n",
    "selected_conditions = [\"Graves' disease\", \"Normal\"]\n",
    "graves_disease_and_normal_data = df[df[\"Diagnosis\"].isin(selected_conditions)]\n",
    "\n",
    "# Dosya adını oluşturma\n",
    "file_name_selected = \"E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\Graves_and_Normal_String_CBC_Data.csv\"\n",
    "\n",
    "\"\"\"\n",
    "# Veriyi CSV olarak kaydetme\n",
    "graves_disease_and_normal_data.to_csv(file_name_selected, index=False)\n",
    "\n",
    "print(f\"CSV file created for 'Graves' Disease and 'Normal': {file_name_selected}\")\n",
    "\"\"\"\n",
    "\n",
    "graves_disease_and_normal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0ab24",
   "metadata": {},
   "source": [
    "**Rounded_Float_Columns_Autoimmune_Dataset.csv** dosyasındaki verileri kullanarak Graves' Hastalığı için ayrı bir csv dosyası oluşturdum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e881ea",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ca211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis değerlerini normalize et (küçük harfe çevir, boşlukları temizle)\n",
    "graves_disease_and_normal_data['Diagnosis'] = graves_disease_and_normal_data['Diagnosis'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Graves' Disease ve Normal hastaları ayırma\n",
    "normal_group = graves_disease_and_normal_data[graves_disease_and_normal_data['Diagnosis'] == \"normal\"]\n",
    "graves_group = graves_disease_and_normal_data[graves_disease_and_normal_data['Diagnosis'] == \"graves' disease\"]\n",
    "\n",
    "# Eğer herhangi bir grup boşsa, hata almamak için kontrol ekleyelim\n",
    "if normal_group.empty:\n",
    "    print(\"Hata: Normal grubu için veri bulunamadı!\")\n",
    "if graves_group.empty:\n",
    "    print(\"Hata: Graves' Disease grubu için veri bulunamadı!\")\n",
    "\n",
    "# Temel istatistikleri hesaplama\n",
    "summary_stats_graves = graves_group.describe()\n",
    "summary_stats_normal = normal_group.describe()\n",
    "\n",
    "# Sonuçları kullanıcıya gösterme\n",
    "import ace_tools_open as tools\n",
    "tools.display_dataframe_to_user(name=\"Graves için Temel İstatistikler\", dataframe=summary_stats_graves)\n",
    "tools.display_dataframe_to_user(name=\"Normal için Temel İstatistikler\", dataframe=summary_stats_normal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f366bf",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korelasyon matrisini hesaplama\n",
    "numeric_df = graves_disease_and_normal_data.select_dtypes(include=[\"number\"])  # Sadece sayısal sütunları seç\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Korelasyon matrisini görselleştirme\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Korelasyon Matrisi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6fb33",
   "metadata": {},
   "source": [
    "## Histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ae6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sayısal değişkenlerin histogramlarını çizme\n",
    "numeric_df.hist(figsize=(15, 12), bins=30, edgecolor=\"black\")\n",
    "plt.suptitle(\"Sayısal Değişkenlerin Dağılımı\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f16cd1",
   "metadata": {},
   "source": [
    "### Patients Count for Graves Disease and Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_conditions = [\"Graves' disease\", \"Normal\"]\n",
    "graves_disease_and_normal_data = df[df[\"Diagnosis\"].isin(selected_conditions)]\n",
    "\n",
    "# Diagnosis değişkeni için Count Plot oluşturma\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.countplot(x=graves_disease_and_normal_data[\"Diagnosis\"], palette='Set2')\n",
    "\n",
    "# Başlık ve etiketleri ekleyelim\n",
    "plt.title('Target Variable: Diagnosis', fontsize=18, fontweight='bold')\n",
    "plt.xlabel(\"Diagnosis\", fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Sütunların üstüne sayıları ekleme\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), \n",
    "                       ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5), \n",
    "                       textcoords='offset points')\n",
    "\n",
    "# Grid kaldır\n",
    "plt.grid(False)\n",
    "\n",
    "# Grafiği göster\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121292e6",
   "metadata": {},
   "source": [
    "## Graves ve Normal Grubun CBC Değerleri Grafikleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f53337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Diagnosis column\n",
    "graves_disease_and_normal_data['Diagnosis'] = graves_disease_and_normal_data['Diagnosis'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Kullanıcıdan alt klasör dizinini belirtmesini isteyin\n",
    "output_folder = 'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\'\n",
    "output_folder = os.path.join(output_folder, \"Graves_and_Normal_Analysis\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Y ekseni için birden fazla değişken tanımlayın\n",
    "y_axis_variables = [\n",
    "'Gender', 'Diagnosis', \"RBC_Count\", \"Hemoglobin\",\"Hematocrit\", \"CRP\", \"Esbach\",\n",
    "\"Anti-Tg\", \"Anti-TPO\", \"Dry eyes and mouth\", \"Joint pain\",\"ACPA\",\"Fatigue or chronic tiredness\"\n",
    "]\n",
    "\n",
    "# Define a function to clean file names\n",
    "def clean_filename(name):\n",
    "    return name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\").replace(\":\", \"_\").replace(\"?\", \"_\").replace(\"*\", \"_\")\n",
    "\n",
    "# Plot each Y-axis variable and save individual Excel tables\n",
    "for y_var in y_axis_variables:\n",
    "    if y_var in graves_disease_and_normal_data.columns:\n",
    "        # Create the bar plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Diagnosis', y=y_var, data=graves_disease_and_normal_data, palette='Set2', ci=None)\n",
    "\n",
    "        # Add title and labels\n",
    "        plt.title(f'{y_var} by Diagnosis', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Diagnosis', fontsize=12)\n",
    "        plt.ylabel(y_var, fontsize=12)\n",
    "\n",
    "        # Annotate bar plot with values\n",
    "        for p in plt.gca().patches:\n",
    "            height = p.get_height()\n",
    "            if not pd.isna(height):  # Ensure height is not NaN\n",
    "                plt.gca().annotate(f'{height:.2f}', (p.get_x() + p.get_width() / 2., height), \n",
    "                                   ha='center', va='baseline', fontsize=10, color='black', xytext=(0, 5), \n",
    "                                   textcoords='offset points')\n",
    "\n",
    "        # Clean the variable name for safe file naming\n",
    "        clean_y_var = clean_filename(y_var)\n",
    "\n",
    "        \"\"\"\n",
    "        # Save the plot as an image\n",
    "        plot_file_path = os.path.join(output_folder, f\"{clean_y_var}_by_Diagnosis.png\")\n",
    "        plt.savefig(plot_file_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Save the corresponding data to an Excel file\n",
    "        data_to_save = graves_disease_and_normal_data.groupby('Diagnosis')[y_var].mean().reset_index()\n",
    "        excel_file_path = os.path.join(output_folder, f\"{clean_y_var}_by_Diagnosis.xlsx\")\n",
    "        data_to_save.to_excel(excel_file_path, index=False)\n",
    "\n",
    "        print(f\"Graph saved at: {plot_file_path}\")\n",
    "        print(f\"Data table saved at: {excel_file_path}\")\n",
    "\n",
    "print(f\"All graphs and individual tables have been saved in the folder: {output_folder}\")\n",
    "\"\"\"\n",
    "\n",
    "graves_disease_and_normal_data_string = graves_disease_and_normal_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19546ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graves_disease_and_normal_data_string['Diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750556c6",
   "metadata": {},
   "source": [
    "## CBC Values for Gender with Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954f7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_and_plot_gender_groups_with_cbc_values(graves_disease_and_normal_data, target_column, save_tables=False, save_plots=False, output_path=None):\n",
    "    \"\"\"\n",
    "    Gender gruplarına göre CBC değerlerini analiz eder, ortalama ve standart sapmalarını hesaplar.\n",
    "    Tabloyu kaydeder ve grafik oluşturur.\n",
    "    \n",
    "    Parametreler:\n",
    "    - graves_disease_and_normal_data: Pandas DataFrame, analiz edilecek veri seti.\n",
    "    - target_column: Hedef sütun (örn: \"Diagnosis\").\n",
    "    - save_tables: Tabloları kaydetmek için True/False.\n",
    "    - save_plots: Grafikleri kaydetmek için True/False.\n",
    "    - output_path: Çıktının kaydedileceği ana klasör.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sadece float sütunları seç\n",
    "    float_columns = graves_disease_and_normal_data.select_dtypes(include=['float']).columns\n",
    "\n",
    "    # Binary sütunları çıkar (sadece 0 ve 1 içerenleri tespit et)\n",
    "    binary_columns = [col for col in float_columns if graves_disease_and_normal_data[col].dropna().nunique() == 2]\n",
    "    float_columns = [col for col in float_columns if col not in binary_columns]  # Binary olanları çıkart\n",
    "\n",
    "    # Çıktı klasörünü belirle\n",
    "    if output_path:\n",
    "        output_folder = os.path.join(output_path, \"CBC_Values_by_Gender\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    else:\n",
    "        raise ValueError(\"Output path cannot be None or empty.\")\n",
    "\n",
    "    # İşlem yapılan dosyaların kaydedildiği dizinleri takip etmek için liste\n",
    "    saved_files = []\n",
    "\n",
    "    # Her float sütun için analiz yap\n",
    "    for col in float_columns:\n",
    "        # Cinsiyete (Gender) göre gruplama ve ortalama & standart sapma hesaplama\n",
    "        grouped_data = graves_disease_and_normal_data.groupby('Gender')[col].agg(['mean', 'std']).reset_index()\n",
    "        grouped_data.columns = ['Gender', f'{col}_mean', f'{col}_std']  # Sütun isimlerini düzenle\n",
    "\n",
    "        # Tabloyu CSV veya Excel olarak kaydet\n",
    "        if save_tables:\n",
    "            output_file = os.path.join(output_folder, f\"{col}_gender_group_table.xlsx\")\n",
    "            grouped_data.to_excel(output_file, index=False)\n",
    "            saved_files.append(output_file)\n",
    "\n",
    "        # Grafik çizimi ve kaydetme\n",
    "        if save_plots:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Hata çubuklarıyla ortalama değerleri gösteren çubuk grafik çizimi\n",
    "            plt.bar(\n",
    "                grouped_data['Gender'], \n",
    "                grouped_data[f'{col}_mean'], \n",
    "                yerr=grouped_data[f'{col}_std'], \n",
    "                capsize=5, \n",
    "                color=['#4C72B0', '#C44E52'],  # Erkek ve kadın için farklı renkler\n",
    "                alpha=0.7\n",
    "            )\n",
    "\n",
    "            # Başlık ve etiketler\n",
    "            plt.title(f'{col} Levels by Gender', fontsize=16)\n",
    "            plt.xlabel('Gender', fontsize=12)\n",
    "            plt.ylabel(f'{col} Level', fontsize=12)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            \"\"\"\n",
    "            # Grafik dosyasını kaydetme\n",
    "            plot_file = os.path.join(output_folder, f\"{col}_plot_by_gender.png\")\n",
    "            plt.savefig(plot_file, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            saved_files.append(plot_file)\n",
    "\n",
    "    # İşlem tamamlandığında kaydedilen dosyaların listesini yazdır\n",
    "    print(f\"Files saved to {output_folder}:\\n\" + \"\\n\".join(saved_files))\n",
    "    \"\"\"\n",
    "\n",
    "# Kullanım\n",
    "output_path = \"E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\\"\n",
    "\n",
    "save_and_plot_gender_groups_with_cbc_values(\n",
    "    graves_disease_and_normal_data,\n",
    "    target_column='Diagnosis',\n",
    "    save_tables=True,\n",
    "    save_plots=True,\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b427e",
   "metadata": {},
   "source": [
    "## Graves için Her Bir Gruptaki Veri Sayısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis sütunundaki boşlukları ve harf büyüklüğünü normalize et\n",
    "graves_disease_and_normal_data['Diagnosis'] = graves_disease_and_normal_data['Diagnosis'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Sadece Graves' disease grubunu seç\n",
    "graves_data = graves_disease_and_normal_data[graves_disease_and_normal_data['Diagnosis'] == \"graves' disease\"]\n",
    "\n",
    "# İncelenecek CBC değişkenlerini tanımla\n",
    "cbc_variables = ['Gender', 'Diagnosis', \"RBC_Count\", \"Hemoglobin\", \"Hematocrit\", \"CRP\", \"Esbach\",\n",
    "                 \"Anti-Tg\", \"Anti-TPO\", \"Dry eyes and mouth\", \"Joint pain\", \"ACPA\", \"Fatigue or chronic tiredness\"]\n",
    "\n",
    "# Graves' disease grubunda cinsiyete göre non-null değerleri say\n",
    "cbc_counts = graves_data.groupby('Gender')[cbc_variables].count()\n",
    "\n",
    "# Sonuçları ekrana yazdır\n",
    "print(\"Graves' disease grubundaki CBC değişkenlerinin cinsiyete göre dağılımı:\")\n",
    "print(cbc_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91795265",
   "metadata": {},
   "source": [
    "## Normal için Her Bir Gruptaki Veri Sayısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis sütunundaki boşlukları ve harf büyüklüğünü normalize et\n",
    "graves_disease_and_normal_data['Diagnosis'] = graves_disease_and_normal_data['Diagnosis'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Sadece Graves' disease grubunu seç\n",
    "normal_data = graves_disease_and_normal_data[graves_disease_and_normal_data['Diagnosis'] == \"normal\"]\n",
    "\n",
    "# İncelenecek CBC değişkenlerini tanımla\n",
    "cbc_variables = ['Gender', 'Diagnosis', \"RBC_Count\", \"Hemoglobin\", \"Hematocrit\", \"CRP\", \"Esbach\",\n",
    "                 \"Anti-Tg\", \"Anti-TPO\", \"Dry eyes and mouth\", \"Joint pain\", \"ACPA\", \"Fatigue or chronic tiredness\"]\n",
    "\n",
    "# Graves' disease grubunda cinsiyete göre non-null değerleri say\n",
    "cbc_counts = normal_data.groupby('Gender')[cbc_variables].count()\n",
    "\n",
    "# Sonuçları ekrana yazdır\n",
    "print(\"Normal disease grubundaki CBC değişkenlerinin cinsiyete göre dağılımı:\")\n",
    "print(cbc_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecfefa",
   "metadata": {},
   "source": [
    "## Convert from String to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61602a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Gender and Diagnosis columns are strings and normalized\n",
    "graves_disease_and_normal_data['Gender'] = graves_disease_and_normal_data['Gender'].astype(str).str.strip().str.lower()\n",
    "graves_disease_and_normal_data['Diagnosis'] =graves_disease_and_normal_data['Diagnosis'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Mapping for Gender and Diagnosis columns\n",
    "gender_mapping = {'male': 0, 'female': 1}\n",
    "diagnosis_mapping = {\"graves' disease\": 0, 'normal': 1}\n",
    "\n",
    "# Apply the mappings\n",
    "graves_disease_and_normal_data['Gender'] = graves_disease_and_normal_data['Gender'].map(gender_mapping)\n",
    "graves_disease_and_normal_data['Diagnosis'] = graves_disease_and_normal_data['Diagnosis'].map(diagnosis_mapping)\n",
    "\n",
    "# Check for NaN values after mapping\n",
    "if graves_disease_and_normal_data['Diagnosis'].isnull().sum() > 0:\n",
    "    print(\"Warning: Unmapped values found in Diagnosis column!\")\n",
    "    print(graves_disease_and_normal_data['Diagnosis'].unique())  # Display all unique values for debugging\n",
    "\n",
    "# Remove duplicates if any\n",
    "graves_disease_and_normal_data = graves_disease_and_normal_data.drop_duplicates()\n",
    "\n",
    "\"\"\"\n",
    "# Save the updated dataset\n",
    "output_file_path = 'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\Graves_and_Normal_Integer_CBC_Data.csv'\n",
    "graves_disease_and_normal_data.to_csv(output_file_path, index=False)\n",
    "print(f\"File saved: {output_file_path}\")\n",
    "\"\"\"\n",
    "\n",
    "graves_disease_and_normal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e083ffd",
   "metadata": {},
   "source": [
    "# Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nbformat\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ipynbname\n",
    "\n",
    "def save_current_notebook_as_backup(save_directory):\n",
    "    try:\n",
    "        # Find current notebook\n",
    "        notebook_path = ipynbname.path()\n",
    "\n",
    "        # Create backup name with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"cbc_values_statistics_to_gender_{timestamp}.ipynb\"\n",
    "\n",
    "        # Create backup index and save to notebook\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "        new_file_path = os.path.join(save_directory, filename)\n",
    "        with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "            nbformat.write(notebook_content, f)\n",
    "\n",
    "        print(f\"Notebook {new_file_path} olarak kaydedildi.\")\n",
    "        return new_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "\n",
    "# Usage\n",
    "save_current_notebook_as_backup(\"E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb49cf3",
   "metadata": {},
   "source": [
    "## Select Column for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8194587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İstenilen sütunları seçme\n",
    "selected_columns_for_ML = [\"Gender\",\"Diagnosis\",\"CRP\", \"Esbach\",\"Anti-Tg\",\"Anti-TPO\", \n",
    "                           \"Dry eyes and mouth\", \"Joint pain\",\"ACPA\",\"Fatigue or chronic tiredness\"]\n",
    "\n",
    "ML_filtered_df_str = graves_disease_and_normal_data_string[selected_columns_for_ML].copy()\n",
    "\n",
    "\"\"\"\n",
    "# Seçilen sütunlarla yeni bir CSV dosyası oluşturma\n",
    "output_path = 'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\Selected_Columns_for_ML_Autoimmune_Dataset.csv'\n",
    "ML_filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Yeni CSV dosyası oluşturuldu: {output_path}\")\n",
    "\"\"\"\n",
    "\n",
    "ML_filtered_df_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3257ef",
   "metadata": {},
   "source": [
    "## Statistics for ML Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a83b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Diagnosis sütununu normalize et (küçük harfe çevir, boşlukları temizle)\n",
    "graves_disease_and_normal_data['Diagnosis'] = graves_disease_and_normal_data['Diagnosis'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Analiz edilecek değişkenler (sadece veri setinde bulunanlar seçilecek)\n",
    "y_axis_variables = ['Gender', 'Diagnosis', \"RBC_Count\", \"Hemoglobin\", \"Hematocrit\", \"CRP\", \"Esbach\",\n",
    "                 \"Anti-Tg\", \"Anti-TPO\", \"Dry eyes and mouth\", \"Joint pain\", \"ACPA\", \"Fatigue or chronic tiredness\"]\n",
    "\n",
    "# Mevcut sütunları kontrol et, olmayan sütunları çıkar\n",
    "y_axis_variables = [col for col in y_axis_variables if col in graves_disease_and_normal_data.columns]\n",
    "\n",
    "\n",
    "# Tüm sütunları sayısala çevir (yanlış format hatalarını önlemek için)\n",
    "for col in y_axis_variables:\n",
    "    graves_disease_and_normal_data[col] = pd.to_numeric(graves_disease_and_normal_data[col], errors='coerce')\n",
    "\n",
    "# Sonuçları saklamak için boş bir sözlük başlatma\n",
    "results = {\n",
    "    \"Variable\": [],\n",
    "    \"Normal_Mean\": [],\n",
    "    \"Normal_Std\": [],\n",
    "    \"Graves_Mean\": [],\n",
    "    \"Graves_Std\": []\n",
    "}\n",
    "\n",
    "# Diagnosis değerlerini normalize et ve filtreleme yap\n",
    "normal_group = graves_disease_and_normal_data[graves_disease_and_normal_data['Diagnosis'] == \"normal\"]\n",
    "graves_group = graves_disease_and_normal_data[graves_disease_and_normal_data['Diagnosis'] == \"graves' disease\"]\n",
    "\n",
    "# Her değişken için istatistik hesaplama\n",
    "for y_var in y_axis_variables:\n",
    "    if y_var in graves_disease_and_normal_data.columns:\n",
    "        results[\"Variable\"].append(y_var)\n",
    "        results[\"Normal_Mean\"].append(normal_group[y_var].mean())\n",
    "        results[\"Normal_Std\"].append(normal_group[y_var].std())\n",
    "        results[\"Graves_Mean\"].append(graves_group[y_var].mean())\n",
    "        results[\"Graves_Std\"].append(graves_group[y_var].std())\n",
    "\n",
    "# DataFrame oluşturma\n",
    "stats_df = pd.DataFrame(results)\n",
    "\n",
    "# Sonuçları ekrana göster\n",
    "import ace_tools_open as tools\n",
    "tools.display_dataframe_to_user(name=\"Graves and Normal Statistics\", dataframe=stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "graves_disease_and_normal_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8319896",
   "metadata": {},
   "source": [
    "## Columns that Choosen with ML in Gender Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choosen_with_cbc_values(graves_disease_and_normal_data, target_column, save_tables=False, save_plots=False, output_path=None):\n",
    "    \"\"\"\n",
    "    Gender gruplarına göre CBC değerlerini analiz eder, ortalama ve standart sapmalarını hesaplar.\n",
    "    Tabloyu kaydeder ve grafik oluşturur.\n",
    "    \n",
    "    Parametreler:\n",
    "    - graves_disease_and_normal_data: Pandas DataFrame, analiz edilecek veri seti.\n",
    "    - target_column: Hedef sütun (örn: \"Diagnosis\").\n",
    "    - save_tables: Tabloları kaydetmek için True/False.\n",
    "    - save_plots: Grafikleri kaydetmek için True/False.\n",
    "    - output_path: Çıktının kaydedileceği ana klasör.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sadece float sütunları seç\n",
    "    float_columns = graves_disease_and_normal_data.select_dtypes(include=['float']).columns\n",
    "\n",
    "    # Binary sütunları çıkar (sadece 0 ve 1 içerenleri tespit et)\n",
    "    binary_columns = [col for col in float_columns if graves_disease_and_normal_data[col].dropna().nunique() == 2]\n",
    "    float_columns = [col for col in float_columns if col not in binary_columns]  # Binary olanları çıkart\n",
    "\n",
    "    # Çıktı klasörünü belirle\n",
    "    if output_path:\n",
    "        output_folder = os.path.join(output_path, \"CBC_Values_by_Gender\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    else:\n",
    "        raise ValueError(\"Output path cannot be None or empty.\")\n",
    "\n",
    "    # İşlem yapılan dosyaların kaydedildiği dizinleri takip etmek için liste\n",
    "    saved_files = []\n",
    "\n",
    "    # Her float sütun için analiz yap\n",
    "    for col in float_columns:\n",
    "        # Cinsiyete (Gender) göre gruplama ve ortalama & standart sapma hesaplama\n",
    "        grouped_data = graves_disease_and_normal_data.groupby('Gender')[col].agg(['mean', 'std']).reset_index()\n",
    "        grouped_data.columns = ['Gender', f'{col}_mean', f'{col}_std']  # Sütun isimlerini düzenle\n",
    "\n",
    "        # Tabloyu CSV veya Excel olarak kaydet\n",
    "        if save_tables:\n",
    "            output_file = os.path.join(output_folder, f\"{col}_gender_group_table.xlsx\")\n",
    "            grouped_data.to_excel(output_file, index=False)\n",
    "            saved_files.append(output_file)\n",
    "\n",
    "        # Grafik çizimi ve kaydetme\n",
    "        if save_plots:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Hata çubuklarıyla ortalama değerleri gösteren çubuk grafik çizimi\n",
    "            plt.bar(\n",
    "                grouped_data['Gender'], \n",
    "                grouped_data[f'{col}_mean'], \n",
    "                yerr=grouped_data[f'{col}_std'], \n",
    "                capsize=5, \n",
    "                color=['#4C72B0', '#C44E52'],  # Erkek ve kadın için farklı renkler\n",
    "                alpha=0.7\n",
    "            )\n",
    "\n",
    "            \"\"\"\n",
    "            # Başlık ve etiketler\n",
    "            plt.title(f'{col} Levels by Gender', fontsize=16)\n",
    "            plt.xlabel('Gender', fontsize=12)\n",
    "            plt.ylabel(f'{col} Level', fontsize=12)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Grafik dosyasını kaydetme\n",
    "            plot_file = os.path.join(output_folder, f\"{col}_plot_by_gender.png\")\n",
    "            plt.savefig(plot_file, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            saved_files.append(plot_file)\n",
    "\n",
    "    # İşlem tamamlandığında kaydedilen dosyaların listesini yazdır\n",
    "    print(f\"Files saved to {output_folder}:\\n\" + \"\\n\".join(saved_files))\n",
    "    \"\"\"\n",
    "\n",
    "# Kullanım\n",
    "output_path = \"E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github\\\\\"\n",
    "\n",
    "choosen_with_cbc_values(\n",
    "    graves_disease_and_normal_data,\n",
    "    target_column='Diagnosis',\n",
    "    save_tables=True,\n",
    "    save_plots=True,\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d443c3",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385387c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"Diagnosis\",\"CRP\", \"Esbach\",\"Anti-Tg\",\"Anti-TPO\", \"Dry eyes and mouth\", \"Joint pain\",\"ACPA\",\n",
    "            \"Fatigue or chronic tiredness\"]\n",
    "\n",
    "# Check for columns that exist in the DataFrame\n",
    "num_cols = [col for col in num_cols if col in graves_disease_and_normal_data.columns]\n",
    "\n",
    "corr = graves_disease_and_normal_data[num_cols].corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ea5e2",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbf762",
   "metadata": {},
   "source": [
    "**General Description:**<br/>\n",
    "\n",
    "1. Matrix Structure:\n",
    "\n",
    "- Each cell in the heatmap represents the correlation coefficient between two variables.<br/>\n",
    "- Variables are listed along both the x-axis and y-axis.<br/>\n",
    "\n",
    "2. Color Scheme (cmap=\"coolwarm\"):\n",
    "\n",
    "The RdBu colormap is used:<br/>\n",
    "- Blue (Positive Correlation): Indicates that two variables are positively correlated (as one increases, the other also tends to increase).<br/>\n",
    "- Red (Negative Correlation): Indicates a negative correlation (as one increases, the other tends to decrease).<br/>\n",
    "- White/Light Shades: Near-zero correlations, implying no strong linear relationship.<br/>\n",
    "\n",
    "3. Annotations (annot=corr_values):<br/>\n",
    "\n",
    "-Each cell is annotated with the numerical value of the correlation coefficient (rounded to two decimal places).\n",
    "\n",
    "**Key Correlation Coefficients:**<br/>\n",
    "\n",
    "1. Range of Values:\n",
    "\n",
    "Values range from -1 to 1:<br/>\n",
    "   - +1: Perfect positive linear relationship.<br/>\n",
    "   - -1: Perfect negative linear relationship.<br/>\n",
    "   -  0: No linear relationship.<br/>\n",
    "     \n",
    "2. Strong Correlations:\n",
    "\n",
    "- Cells with values close to +1 or -1 (intense red or blue shades) indicate strong relationships.<br/>\n",
    "- Look for clusters of strong correlations to identify groups of variables that are highly related.\n",
    "\n",
    "3. Weak/No Correlations:\n",
    "\n",
    "- Cells near 0 (white or light shades) suggest weak or no linear relationship between the variables.\n",
    "\n",
    "**How to Use the Heatmap:**<br/>\n",
    "\n",
    "1. Identify Highly Correlated Variables:\n",
    "\n",
    "- Strong positive correlations (e.g., close to +1) suggest variables that might contain redundant information.\n",
    "- Strong negative correlations (e.g., close to -1) suggest inverse relationships.<br/>\n",
    "\n",
    "2. Feature Selection:\n",
    "\n",
    "- If two variables are strongly correlated, consider removing one from your model to reduce multicollinearity.\n",
    "\n",
    "3. Data Relationships:\n",
    "\n",
    "- The heatmap helps you quickly spot trends and relationships in the data that might not be immediately apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943bf27",
   "metadata": {},
   "source": [
    "# TENSORFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248add8",
   "metadata": {},
   "source": [
    "## Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f0d4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "raw_csv_data = pd.read_csv('E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github-Graves Disease\\\\Graves_and_Normal_Integer_CBC_Data.csv')\n",
    "\n",
    "# The inputs are all columns in the csv, except for the first one [:,0]\n",
    "# (which is just the arbitrary customer IDs that bear no useful information),\n",
    "# and the last one [:,-1] (which is our targets)\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data.iloc[:, 1:-1].values\n",
    "\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "targets_all = raw_csv_data.iloc[:,-1].values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13440e",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3860abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Count how many targets are 1 (meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "# Set a counter for targets that are 0 (meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
    "# Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "# Count the number of targets that are 0. \n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "# We delete all indices that we marked \"to remove\" in the loop above.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85113e",
   "metadata": {},
   "source": [
    "## Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09b11d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities\n",
    "# It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.\n",
    "# At the end of the business case, you can try to run the algorithm WITHOUT this line of code. \n",
    "# The result will be interesting.\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426718e",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9fad193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caa465",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40ff9744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1841 3537 0.5204975968334747\n",
      "239 442 0.5407239819004525\n",
      "245 443 0.5530474040632054\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e53c0",
   "metadata": {},
   "source": [
    "## Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab2a03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the three datasets in *.npz.\n",
    "# In the next lesson, you will see that it is extremely valuable to name them in such a coherent way!\n",
    "\"\"\"\n",
    "np.savez('AutoImmuneDisease_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('AutoImmuneDisease_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('AutoImmuneDisease_data_test', inputs=test_inputs, targets=test_targets)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a01011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('AutoImmuneDisease_data_train.npz')\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('AutoImmuneDisease_data_validation.npz')\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('AutoImmuneDisease_data_test.npz')\n",
    "# we create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95855246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 2s - 49ms/step - accuracy: 0.6127 - loss: 0.6340 - val_accuracy: 0.6946 - val_loss: 0.5791\n",
      "Epoch 2/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.7334 - loss: 0.5491 - val_accuracy: 0.7557 - val_loss: 0.5201\n",
      "Epoch 3/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.7908 - loss: 0.4887 - val_accuracy: 0.8145 - val_loss: 0.4639\n",
      "Epoch 4/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8329 - loss: 0.4336 - val_accuracy: 0.8235 - val_loss: 0.4110\n",
      "Epoch 5/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8606 - loss: 0.3772 - val_accuracy: 0.8665 - val_loss: 0.3529\n",
      "Epoch 6/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.8892 - loss: 0.3232 - val_accuracy: 0.8937 - val_loss: 0.3108\n",
      "Epoch 7/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9050 - loss: 0.2780 - val_accuracy: 0.9118 - val_loss: 0.2587\n",
      "Epoch 8/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9174 - loss: 0.2363 - val_accuracy: 0.9412 - val_loss: 0.2196\n",
      "Epoch 9/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9384 - loss: 0.2058 - val_accuracy: 0.9412 - val_loss: 0.1946\n",
      "Epoch 10/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9511 - loss: 0.1741 - val_accuracy: 0.9570 - val_loss: 0.1638\n",
      "Epoch 11/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9567 - loss: 0.1525 - val_accuracy: 0.9615 - val_loss: 0.1430\n",
      "Epoch 12/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9604 - loss: 0.1346 - val_accuracy: 0.9593 - val_loss: 0.1286\n",
      "Epoch 13/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9621 - loss: 0.1210 - val_accuracy: 0.9525 - val_loss: 0.1244\n",
      "Epoch 14/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9630 - loss: 0.1127 - val_accuracy: 0.9661 - val_loss: 0.1065\n",
      "Epoch 15/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9652 - loss: 0.1009 - val_accuracy: 0.9638 - val_loss: 0.0992\n",
      "Epoch 16/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9655 - loss: 0.0945 - val_accuracy: 0.9593 - val_loss: 0.0952\n",
      "Epoch 17/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9658 - loss: 0.0898 - val_accuracy: 0.9706 - val_loss: 0.0888\n",
      "Epoch 18/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9675 - loss: 0.0856 - val_accuracy: 0.9661 - val_loss: 0.0849\n",
      "Epoch 19/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9675 - loss: 0.0814 - val_accuracy: 0.9593 - val_loss: 0.0842\n",
      "Epoch 20/100\n",
      "36/36 - 0s - 2ms/step - accuracy: 0.9681 - loss: 0.0795 - val_accuracy: 0.9593 - val_loss: 0.0814\n",
      "Epoch 21/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9714 - loss: 0.0757 - val_accuracy: 0.9661 - val_loss: 0.0778\n",
      "Epoch 22/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9700 - loss: 0.0743 - val_accuracy: 0.9706 - val_loss: 0.0794\n",
      "Epoch 23/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.9729 - loss: 0.0723 - val_accuracy: 0.9638 - val_loss: 0.0792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13f7c785df0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "### Choose the optimizer and the loss function\n",
    "\n",
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "### Training\n",
    "# That's where we train the model we have built.\n",
    "\n",
    "# set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# set a maximum number of training epochs\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "# fit the model\n",
    "# note that this time the train, validation and test data are not iterable\n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          # callbacks are functions called by a task when a task is completed\n",
    "          # task here is to check if val_loss is increasing\n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8eaa3b",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "473ba710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.9525 - loss: 0.0938\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd7c1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.10. Test accuracy: 95.26%\n"
     ]
    }
   ],
   "source": [
    "# print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d7a11",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb28e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook E:\\Veri Bilimi Topluluğu\\proje-AID\\github-Graves Disease\\AID_Prediction_with_tensor_flow_20250131_134055.ipynb olarak kaydedildi.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github-Graves Disease\\\\AID_Prediction_with_tensor_flow_20250131_134055.ipynb'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import nbformat\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ipynbname\n",
    "\n",
    "def save_current_notebook_as_backup(save_directory):\n",
    "    try:\n",
    "        # Find current notebook\n",
    "        notebook_path = ipynbname.path()\n",
    "\n",
    "        # Create backup name with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"AID_Prediction_with_tensor_flow_{timestamp}.ipynb\"\n",
    "\n",
    "        # Create backup index and save to notebook\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "        new_file_path = os.path.join(save_directory, filename)\n",
    "        with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "            nbformat.write(notebook_content, f)\n",
    "\n",
    "        print(f\"Notebook {new_file_path} olarak kaydedildi.\")\n",
    "        return new_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "\n",
    "# Usage\n",
    "save_current_notebook_as_backup(\"E:\\\\Veri Bilimi Topluluğu\\\\proje-AID\\\\github-Graves Disease\\\\\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32819b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
